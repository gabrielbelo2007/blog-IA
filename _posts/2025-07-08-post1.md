---
layout: post
image: assets/img/post_07_08/header_post_07_08.png
image_caption: Uma imagem ilustrativa do avanço dos Agentes de IA
title: "Do Zero ao Um: Aprendendo Modelos de Agente"
subtitle: Como construir sistemas agênticos robustos e eficazes?
date: 2025-07-08
author: Philipp Schmid
tradutor: Gabriel A. Belo
categories: [Agentes de IA]
tags: [IA Generativa, Chatbots, Agentes de IA]
---

Agentes de IA. IA Agêntica. Arquitetura Agêntica. *Workflow* Agênticos. Modelos Agênticos. Agentes estão em todos os lugares. Mas o que exatamente são eles, e como construir sistemas agênticos robustos e eficazes? Enquanto o termo "agente" é amplamente usado, uma característica fundamental é a sua capacidade de planejar e executar tarefas dinamicamente, frequentemente utilizando **ferramentas externas** e **memória** para alcançar objetivos complexos.

Este post tem como objetivo explorar modelos de design comuns. Visualizar essas estruturas como padrões reutilizáveis para construir aplicações de IA. Compreendê-los oferece um modelo mental para solucionar problemas complexos e projetar sistemas que sejam escaláveis, modulares e adaptáveis.

Vamos aprofundar em diversos padrões comuns diferenciando entre *workflows* mais estruturados e padrões agênticos mais dinâmicos. *Workflows* geralmente seguem caminhos predefinidos, enquanto agentes possuem mais autonomia para decidir seu curso de ação.

### Por que "Padrões Agênticos" importam?

- Padrões oferecem uma maneira estruturada de pensar e projetar sistemas.

- Padrões nos permitem construir e expandir aplicações de IA em complexidade e adaptar a requisitos variáveis. Designs modulares baseados em padrões são mais fáceis de modificar e estender.

- Padrões ajudam a gerenciar a complexidade da coordenação de múltiplos agentes, ferramentas e fluxos de trabalho, oferecendo modelos reutilizáveis e confiáveis. Eles promovem as melhores práticas e o entendimento compartilhado entre os desenvolvedores.

### Quando (e quando não) usar Agentes?

Antes de mergulharmos nos padrões, é crucial considerar quando uma abordagem agêntica é realmente necessária.

- Sempre busque a solução mais simples primeiro. Se você souber os passos exatos necessários para resolver um problema, um fluxo de trabalho fixo ou até mesmo um script simples pode ser mais eficiente e acertivo do que um agente.

- Sistemas agênticos frequentemente trocam o aumento da latência e do custo computacional por um desempenho potencialmente melhor em tarefas complexas, ambíguas ou dinâmicas. Certifique-se de que os benefícios superam esses custos.

- Use *workflows* para previsibilidade e consistência ao lidar com tarefas bem definidas onde os passos são conhecidos.

- Use agentes quando flexibilidade, adaptabilidade e tomadas de decisões orientadas por modelos são necessárias.

- Mantenha a Simplicidade (ainda que com agentes): Mesmo ao construir sistemas agênticos, esforce-se pelo design mais simples e eficaz. Agentes excessivamente complexos podem se tornar difíceis de depurar e gerenciar.

- Agentes introduzem imprevisibilidade inerente e potenciais erros. Sistemas agênticos devem incorporar registro de erros robusto, tratamento de exceções e mecanismos de repetição, dando ao sistema (ou ao LLM subjacente) a chance de se autocorrigir.

Abaixo, exploraremos 3 padrões de *workflows* comuns e 4 padrões agênticos. Ilustraremos cada um usando chamadas de API puras, sem depender de frameworks específicos como LangChain, LangGraph, LlamaIndex ou CrewAI, para focar nos conceitos centrais.

### Visão Geral dos Padrões

Vamos cobrir os seguintes modelos:

- [*Workflow*: Prompt encadeado](#workflow-prompt-encadeado)
- [*Workflow*: Roteamento ou *Handoff*](#workflow-roteamento-ou-handoff)
- [*Workflow*: Paralelização](#workflow-paralelização)
- [Padrão de Reflexão](#padrão-de-reflexão)
- [Padrão de Uso de Ferramentas](#padrão-de-uso-de-ferramentas)
- [Padrão: Planejamento (Orquestrador-Trabalhadores)](#padrão-planejamento-orquestrador-trabalhadores)
- [Padrão: Multiagente](#padrão-multiagente)

### *Workflow*: Prompt encadeado

![prompt-encadeado]({{ site.baseurl }}/assets/img/post_07_08/prompt-chaining.png)

A saída de uma chamada de *LLM* alimenta sequencialmente a entrada da próxima chamada de *LLM*. Esse padrão decompõe uma tarefa em uma sequência fixa de passos. Cada passo é tratado por uma chamada de *LLM* que processa a saída da anterior. É adequado para tarefas que podem ser divididas de forma clara em subtarefas previsíveis e sequenciais.

#### Casos de Uso

- **Gerar um documento estruturado**: *LLM 1* cria um esboço, *LLM 2* valida o esboço contra critérios, *LLM 3* escreve o conteúdo com base no esboço validado.

- **Processamento de dados em várias etapas**: Extrair informações, transformá-las e depois resumi-las.

- **Gerar conjunto de notícias com base em entradas curadas**.

```python

import os
from google import genai
 
# Configure the client (ensure GEMINI_API_KEY is set in your environment)
client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
 
# --- Step 1: Summarize Text ---
original_text = "Large language models are powerful AI systems trained on vast amounts of text data. They can generate human-like text, translate languages, write different kinds of creative content, and answer your questions in an informative way."
prompt1 = f"Summarize the following text in one sentence: {original_text}"
 
# Use client.models.generate_content
response1 = client.models.generate_content(
    model='gemini-2.0-flash',
    contents=prompt1
)
summary = response1.text.strip()
print(f"Summary: {summary}")
 
# --- Step 2: Translate the Summary ---
prompt2 = f"Translate the following summary into French, only return the translation, no other text: {summary}"
 
# Use client.models.generate_content
response2 = client.models.generate_content(
    model='gemini-2.0-flash',
    contents=prompt2
)
translation = response2.text.strip()
print(f"Translation: {translation}")

```

### *Workflow*: Roteamento ou Handoff

![routeamento]({{ site.baseurl }}/assets/img/post_07_08/routing-or-handoff.png)

Um *LLM* inicial atua como um roteador, classificando a entrada do usuário e direcionando-a para a tarefa especializada ou *LLM* mais apropriada. Esse padrão implementa uma separação de responsabilidades e permite otimizar tarefas individuais (usando prompts especializados, modelos diferentes ou ferramentas específicas) isoladamente. Ele melhora a eficiência e potencialmente reduz custos ao usar modelos menores para tarefas mais simples. Quando uma tarefa é roteada, o agente selecionado "assume" a responsabilidade pela conclusão.

> O termo *"handoff"* é uma expressão americana que significa a passagem de responsabilidades de um funcionário ou departamento para outro mais qualificado.

#### Casos de Uso

- **Sistemas de suporte ao cliente**: Roteamento de consultas para agentes especializados em cobrança, suporte técnico ou informações de produtos.

- **Uso de LLM em camadas**: Roteamento de consultas simples para modelos mais rápidos e baratos (como Llama 3.1 8B) e perguntas complexas ou incomuns para modelos mais capazes (como Gemini 1.5 Pro).

- **Geração de conteúdo**: Roteamento de solicitações para postagens de blog, atualizações de mídias sociais ou textos de anúncios para diferentes prompts/modelos especializados.

```python

import os
import json
from google import genai
from pydantic import BaseModel
import enum
 
# Configure the client (ensure GEMINI_API_KEY is set in your environment)
client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
 
# Define Routing Schema
class Category(enum.Enum):
    WEATHER = "weather"
    SCIENCE = "science"
    UNKNOWN = "unknown"
 
class RoutingDecision(BaseModel):
    category: Category
    reasoning: str
 
# Step 1: Route the Query
user_query = "What's the weather like in Paris?"
# user_query = "Explain quantum physics simply."
# user_query = "What is the capital of France?"
 
prompt_router = f"""
Analyze the user query below and determine its category.
Categories:
- weather: For questions about weather conditions.
- science: For questions about science.
- unknown: If the category is unclear.
 
Query: {user_query}
"""
 
# Use client.models.generate_content with config for structured output
response_router = client.models.generate_content(
    model= 'gemini-2.0-flash-lite',
    contents=prompt_router,
    config={
        'response_mime_type': 'application/json',
        'response_schema': RoutingDecision,
    },
)
print(f"Routing Decision: Category={response_router.parsed.category}, Reasoning={response_router.parsed.reasoning}")
 
# Step 2: Handoff based on Routing
final_response = ""
if response_router.parsed.category == Category.WEATHER:
    weather_prompt = f"Provide a brief weather forecast for the location mentioned in: '{user_query}'"
    weather_response = client.models.generate_content(
        model='gemini-2.0-flash',
        contents=weather_prompt
    )
    final_response = weather_response.text
elif response_router.parsed.category == Category.SCIENCE:
    science_response = client.models.generate_content(
        model="gemini-2.5-flash-preview-04-17",
        contents=user_query
    )
    final_response = science_response.text
else:
    unknown_response = client.models.generate_content(
        model="gemini-2.0-flash-lite",
        contents=f"The user query is: {prompt_router}, but could not be answered. Here is the reasoning: {response_router.parsed.reasoning}. Write a helpful response to the user for him to try again."
    )
    final_response = unknown_response.text
print(f"\nFinal Response: {final_response}")

```

### *Workflow*: Paralelização

![paralelização]({{ site.baseurl }}/assets/img/post_07_08/parallelization.png)

Uma tarefa é dividida em subtarefas independentes que são processadas simultaneamente por múltiplos *LLMs*, com suas saídas sendo agregadas. Esse padrão utiliza a concorrência para as tarefas. A consulta inicial (ou partes dela) é enviada para múltiplos *LLMs* em paralelo com prompts/objetivos individuais. Uma vez que todos os fluxos são concluídos, seus resultados individuais são coletados e passados para um *LLM* agregador final, que os sintetiza a resposta final. Isso pode melhorar a latência se as subtarefas não dependerem umas das outras, ou aumentar a qualidade por meio de técnicas como votação majoritária ou geração de opções diversas.

#### Casos de Uso

- **RAG com decomposição de consulta**: Quebrar uma consulta complexa em subconsultas, executar recuperações para cada uma em paralelo e sintetizar os resultados.

- **Análise de documentos grandes**: Dividir o documento em seções, resumir cada seção em paralelo e, em seguida, combinar os resumos.

- **Geração de múltiplas perspectivas**: Fazer a mesma pergunta a vários LLMs com diferentes prompts de persona e agregar suas respostas.

- **Operações de estilo Map-reduce em dados**.

```python

import os
import asyncio
import time
from google import genai
 
# Configure the client (ensure GEMINI_API_KEY is set in your environment)
client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
 
async def generate_content(prompt: str) -> str:
        response = await client.aio.models.generate_content(
            model="gemini-2.0-flash",
            contents=prompt
        )
        return response.text.strip()
 
async def parallel_tasks():
    # Define Parallel Tasks
    topic = "a friendly robot exploring a jungle"
    prompts = [
        f"Write a short, adventurous story idea about {topic}.",
        f"Write a short, funny story idea about {topic}.",
        f"Write a short, mysterious story idea about {topic}."
    ]
    # Run tasks concurrently and gather results
    start_time = time.time()
    tasks = [generate_content(prompt) for prompt in prompts]
    results = await asyncio.gather(*tasks)
    end_time = time.time()
    print(f"Time taken: {end_time - start_time} seconds")
 
    print("\n--- Individual Results ---")
    for i, result in enumerate(results):
        print(f"Result {i+1}: {result}\n")
 
    # Aggregate results and generate final story
    story_ideas = '\n'.join([f"Idea {i+1}: {result}" for i, result in enumerate(results)])
    aggregation_prompt = f"Combine the following three story ideas into a single, cohesive summary paragraph:{story_ideas}"
    aggregation_response = await client.aio.models.generate_content(
        model="gemini-2.5-flash-preview-04-17",
        contents=aggregation_prompt
    )
    return aggregation_response.text
    
 
result = await parallel_tasks()
print(f"\n--- Aggregated Summary ---\n{result}")

```

### Padrão de Reflexão

![reflexão]({{ site.baseurl }}/assets/img/post_07_08/reflection.png)

Um agente avalia sua própria saída e usa esse feedback para refinar sua resposta iterativamente. Este padrão também é conhecido como Avaliador-Otimizador e utiliza um loop de autocorreção. Um *LLM* inicial gera uma resposta ou completa uma tarefa. Uma segunda etapa do *LLM* (ou até mesmo o mesmo *LLM* com um prompt diferente) atua como um refletor ou avaliador, criticando a saída inicial em relação aos requisitos ou à qualidade desejada. Essa crítica (feedback) é então realimentada, impulsionando o *LLM* a produzir uma saída refinada. Esse ciclo pode se repetir até que o avaliador confirme que os requisitos foram atendidos ou que uma saída satisfatória foi alcançada.

#### Casos de Uso

- **Geração de código**: Escrever código, executá-lo, usar mensagens de erro ou resultados de testes como feedback para corrigir bugs.

- **Escrita e refinamento**: Gerar um rascunho, refletir sobre sua clareza e tom, e depois revisá-lo.

- **Resolução de problemas complexos**: Gerar um plano, avaliar sua viabilidade e refiná-lo com base na avaliação.

- **Recuperação de informações**: Pesquisar informações e usar um LLM avaliador para verificar se todos os detalhes necessários foram encontrados antes de apresentar a resposta.

```python

import os
import json
from google import genai
from pydantic import BaseModel
import enum
 
# Configure the client (ensure GEMINI_API_KEY is set in your environment)
client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
 
class EvaluationStatus(enum.Enum):
    PASS = "PASS"
    FAIL = "FAIL"
 
class Evaluation(BaseModel):
    evaluation: EvaluationStatus
    feedback: str
    reasoning: str
 
# --- Initial Generation Function ---
def generate_poem(topic: str, feedback: str = None) -> str:
    prompt = f"Write a short, four-line poem about {topic}."
    if feedback:
        prompt += f"\nIncorporate this feedback: {feedback}"
    
    response = client.models.generate_content(
        model='gemini-2.0-flash',
        contents=prompt
    )
    poem = response.text.strip()
    print(f"Generated Poem:\n{poem}")
    return poem
 
# --- Evaluation Function ---
def evaluate(poem: str) -> Evaluation:
    print("\n--- Evaluating Poem ---")
    prompt_critique = f"""Critique the following poem. Does it rhyme well? Is it exactly four lines? 
Is it creative? Respond with PASS or FAIL and provide feedback.
 
Poem:
{poem}
"""
    response_critique = client.models.generate_content(
        model='gemini-2.0-flash',
        contents=prompt_critique,
        config={
            'response_mime_type': 'application/json',
            'response_schema': Evaluation,
        },
    )
    critique = response_critique.parsed
    print(f"Evaluation Status: {critique.evaluation}")
    print(f"Evaluation Feedback: {critique.feedback}")
    return critique
 
# Reflection Loop   
max_iterations = 3
current_iteration = 0
topic = "a robot learning to paint"
 
# simulated poem which will not pass the evaluation
current_poem = "With circuits humming, cold and bright,\nA metal hand now holds a brush"
 
while current_iteration < max_iterations:
    current_iteration += 1
    print(f"\n--- Iteration {current_iteration} ---")
    evaluation_result = evaluate(current_poem)
 
    if evaluation_result.evaluation == EvaluationStatus.PASS:
        print("\nFinal Poem:")
        print(current_poem)
        break
    else:
        current_poem = generate_poem(topic, feedback=evaluation_result.feedback)
        if current_iteration == max_iterations:
            print("\nMax iterations reached. Last attempt:")
            print(current_poem)

```

### Padrão de Uso de Ferramentas

![ferramentas]({{ site.baseurl }}/assets/img/post_07_08/tool-use.png)

A *LLM* tem a capacidade de invocar funções ou APIs externas para interagir com o mundo exterior, recuperar informações ou realizar ações. Este padrão, frequentemente referido como Chamada de Função, é o mais amplamente reconhecido. A *LLM* recebe definições (nome, descrição, esquema de entrada) das ferramentas disponíveis (funções, APIs, bancos de dados, etc.). Com base na consulta do usuário, a *LLM* podem decidir chamar uma ou mais ferramentas, gerando uma saída estruturada (como JSON) que corresponde ao esquema exigido. Essa saída é usada para executar a ferramenta/função externa real, e o resultado é retornado para a *LLM*. A *LLM* então usa esse resultado para formular sua resposta final ao usuário. Isso estende vastamente as capacidades do *LLM* além de seus dados de treinamento.

#### Casos de Uso

- **Agendamento de compromissos usando uma API de calendário.**

- **Recuperação de preços de ações em tempo real via uma API financeira.**

- **Busca em um banco de dados vetorial por documentos relevantes (RAG).**

- **Controle de dispositivos de casa inteligente.**

- **Execução de trechos de código.**

```python

import os
from google import genai
from google.genai import types
 
# Configure the client (ensure GEMINI_API_KEY is set in your environment)
client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
 
# Define the function declaration for the model
weather_function = {
    "name": "get_current_temperature",
    "description": "Gets the current temperature for a given location.",
    "parameters": {
        "type": "object",
        "properties": {
            "location": {
                "type": "string",
                "description": "The city name, e.g. San Francisco",
            },
        },
        "required": ["location"],
    },
}
 
# Placeholder function to simulate API call
def get_current_temperature(location: str) -> dict:
    return {"temperature": "15", "unit": "Celsius"}
 
# Create the config object as shown in the user's example
# Use client.models.generate_content with model, contents, and config
tools = types.Tool(function_declarations=[weather_function])
contents = ["What's the temperature in London right now?"]
response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents=contents,
    config = types.GenerateContentConfig(tools=[tools])
)
 
# Process the Response (Check for Function Call)
response_part = response.candidates[0].content.parts[0]
if response_part.function_call:
    function_call = response_part.function_call
    print(f"Function to call: {function_call.name}")
    print(f"Arguments: {dict(function_call.args)}")
 
    # Execute the Function
    if function_call.name == "get_current_temperature":        
        # Call the actual function
        api_result = get_current_temperature(*function_call.args)
        # Append function call and result of the function execution to contents
        follow_up_contents = [
            types.Part(function_call=function_call),
            types.Part.from_function_response(
                name="get_current_temperature",
                response=api_result
            )
        ]
        # Generate final response
        response_final = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=contents + follow_up_contents,
            config=types.GenerateContentConfig(tools=[tools])
        )
        print(response_final.text)
    else:
        print(f"Error: Unknown function call requested: {function_call.name}")
else:
    print("No function call found in the response.")
    print(response.text)

```

### Padrão: Planejamento (Orquestrador-Trabalhadores)

![planejamento]({{ site.baseurl }}/assets/img/post_07_08/planning.png)

Um *LLM* planejador central decompõe uma tarefa complexa em uma lista dinâmica de subtarefas, que são delegadas a agentes trabalhadores especializados (muitas vezes utilizando o **Padrão de Uso de Ferramentas**) para execução. Este padrão tenta resolver problemas complexos que exigem raciocínio em várias etapas, criando um ***Plano inicial***. Este plano é gerado dinamicamente com base na entrada do usuário. As subtarefas são então atribuídas a agentes "Trabalhadores" que as executam, potencialmente em paralelo se as dependências permitirem. Um *LLM* "Orquestrador" ou "Sintetizador" coleta os resultados dos trabalhadores, reflete se o objetivo geral foi alcançado e sintetiza a saída final ou potencialmente inicia uma etapa de replanejamento, se necessário. Isso reduz a carga cognitiva em qualquer chamada de *LLM* única, melhora a qualidade do raciocínio, minimiza erros e permite a adaptação dinâmica do fluxo de trabalho. 

> A principal diferença do **Roteamento** é que o **Planejador** gera um plano de várias etapas, em vez de selecionar uma única próxima etapa.

#### Casos de Uso

- **Tarefas complexas de desenvolvimento de software**: Dividir "construir uma funcionalidade" em subtarefas de planejamento, codificação, teste e documentação.

- **Pesquisa e geração de relatórios**: Planejar etapas como busca de literatura, extração de dados, análise e redação de relatórios.

- **Tarefas multimodais**: Planejar etapas que envolvam geração de imagens, análise de texto e integração de dados.

- **Executar solicitações complexas de usuários**: "Planejar uma viagem de 3 dias para Paris, reservar voos e um hotel dentro do meu orçamento."

```python

import os
from google import genai
from pydantic import BaseModel, Field
from typing import List
 
# Configure the client (ensure GEMINI_API_KEY is set in your environment)
client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
 
# Define the Plan Schema
class Task(BaseModel):
    task_id: int
    description: str
    assigned_to: str = Field(description="Which worker type should handle this? E.g., Researcher, Writer, Coder")
 
class Plan(BaseModel):
    goal: str
    steps: List[Task]
 
# Step 1: Generate the Plan (Planner LLM)
user_goal = "Write a short blog post about the benefits of AI agents."
 
prompt_planner = f"""
Create a step-by-step plan to achieve the following goal. 
Assign each step to a hypothetical worker type (Researcher, Writer).
 
Goal: {user_goal}
"""
 
print(f"Goal: {user_goal}")
print("Generating plan...")
 
# Use a model capable of planning and structured output
response_plan = client.models.generate_content(
    model='gemini-2.5-pro-preview-03-25',
    contents=prompt_planner,
    config={
        'response_mime_type': 'application/json',
        'response_schema': Plan,
    },
)
 
# Step 2: Execute the Plan (Orchestrator/Workers - Omitted for brevity) 
for step in response_plan.parsed.steps:
    print(f"Step {step.task_id}: {step.description} (Assignee: {step.assigned_to})")

```

### Padrão: Multiagente

![multi-agente]({{ site.baseurl }}/assets/img/post_07_08/multi-agent.png)

*Abordagem Coordenador-Gerente*

![multi-agente2]({{ site.baseurl }}/assets/img/post_07_08/multi-agent-2.png)

*Abordagem de enxame*

Múltiplos agentes distintos, cada um com um papel, persona ou especialidade específica, colaboram para atingir um objetivo comum. Este padrão utiliza agentes autônomos ou semi-autônomos. Cada agente pode ter um papel único (por exemplo, Gerente de Projeto, Desenvolvedor, Testador, Crítico), conhecimento especializado ou acesso a ferramentas específicas. Eles interagem e colaboram, frequentemente coordenados por um agente "coordenador" ou "gerente" central (como o **Gerente de Projetos** no diagrama) ou usando **lógica de "handoff"**, onde um agente passa o controle para outro agente.

#### Casos de Uso

- **Simulação de debates ou sessões de brainstorming com diferentes personas de IA.**

- **Criação de software complexo envolvendo agentes para planejamento, codificação, teste e implantação.**

- **Execução de experimentos virtuais ou simulações com agentes representando diferentes atores.**

- **Processos de escrita colaborativa ou criação de conteúdo.**

> O exemplo abaixo é uma simplificação de como usar o **padrão Multi-Agente** com lógica de handoff e saída estruturada. Recomendo acessar [LangGraph Multi-Agent Swarm](https://github.com/langchain-ai/langgraph-swarm-py) ou [Crew AI](https://www.crewai.com/open-source) para implementações mais completas.

```python

from google import genai
from pydantic import BaseModel, Field
 
# Configure the client (ensure GEMINI_API_KEY is set in your environment)
client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
 
# Define Structured Output Schemas
class Response(BaseModel):
    handoff: str = Field(default="", description="The name/role of the agent to hand off to. Available agents: 'Restaurant Agent', 'Hotel Agent'")
    message: str = Field(description="The response message to the user or context for the next agent")
 
# Agent Function
def run_agent(agent_name: str, system_prompt: str, prompt: str) -> Response:
    response = client.models.generate_content(
        model='gemini-2.0-flash',
        contents=prompt,
        config = {'system_instruction': f'You are {agent_name}. {system_prompt}', 'response_mime_type': 'application/json', 'response_schema': Response}
    )
    return response.parsed
 
 
# Define System Prompts for the agents
hotel_system_prompt = "You are a Hotel Booking Agent. You ONLY handle hotel bookings. If the user asks about restaurants, flights, or anything else, respond with a short handoff message containing the original request and set the 'handoff' field to 'Restaurant Agent'. Otherwise, handle the hotel request and leave 'handoff' empty."
restaurant_system_prompt = "You are a Restaurant Booking Agent. You handle restaurant recommendations and bookings based on the user's request provided in the prompt."
 
# Prompt to be about a restaurant
initial_prompt = "Can you book me a table at an Italian restaurant for 2 people tonight?"
print(f"Initial User Request: {initial_prompt}")
 
# Run the first agent (Hotel Agent) to force handoff logic
output = run_agent("Hotel Agent", hotel_system_prompt, initial_prompt)
 
# simulate a user interaction to change the prompt and handoff
if output.handoff == "Restaurant Agent":
    print("Handoff Triggered: Hotel to Restaurant")
    output = run_agent("Restaurant Agent", restaurant_system_prompt, initial_prompt)
elif output.handoff == "Hotel Agent":
    print("Handoff Triggered: Restaurant to Hotel")
    output = run_agent("Hotel Agent", hotel_system_prompt, initial_prompt)
 
print(output.message)    

```

### Combinando e Customizando esses Padrões

É importante lembrar que esses padrões não são regras fixas, mas blocos de construção flexíveis. Sistemas agênticos do mundo real frequentemente combinam elementos de múltiplos padrões. Um **agente de Planejamento** pode usar **Ferramentas**, e seus trabalhadores podem empregar a **Reflexão**. Um sistema **Multiagente** pode usar **Roteamento** internamente para atribuição de tarefas.

A chave para o sucesso com qualquer aplicação de *LLM*, especialmente sistemas agentivos complexos, é a avaliação empírica. Defina métricas, meça o desempenho, identifique gargalos ou pontos de falha e itere no seu design. Resista à superengenharia.

### Reconhecimento

Este *post* foi criado com o auxílio de pesquisa aprofundada e manual, buscando inspiração e informações em diversas fontes excelentes, incluindo:

- [5 Agentic AI Design Patterns](https://blog.dailydoseofds.com/p/5-agentic-ai-design-patterns)
- [What are Agentic Workflows?](https://weaviate.io/blog/what-are-agentic-workflows)
- [Building effective agents](https://www.anthropic.com/engineering/building-effective-agents)
- [How Agents Can Improve LLM Performance](https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/)
- [Agentic Design Patterns](https://medium.com/@bijit211987/agentic-design-patterns-cbd0aae2962f)
- [Agent Recipes](https://www.agentrecipes.com/)
- [LangGraph Agentic Concepts](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)
- [OpenAI Agents Python Examples](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns)
- [Anthropic Cookbook](https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns)

### Créditos

##### Esta postagem é uma tradução autorizada do post *[Zero to One: Learning Agentic Patterns](https://www.philschmid.de/agentic-pattern)*, de autoria de Philipp Schmid, *AI Developer Experience* do Google.

---
